{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:40:52.541594Z",
     "start_time": "2018-04-02T12:17:31.792574Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing***********************************************************************************************\n",
      "data has been imported\n",
      "************************************ training the lst layer... ************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.347165\n",
      "loss:  0.295270\n",
      "loss:  0.278825\n",
      "loss:  0.270724\n",
      "loss:  0.265891\n",
      "loss:  0.262688\n",
      "loss:  0.260407\n",
      "loss:  0.258697\n",
      "loss:  0.257363\n",
      "loss:  0.256295\n",
      "loss:  0.255424\n",
      "loss:  0.254700\n",
      "loss:  0.254089\n",
      "loss:  0.253566\n",
      "loss:  0.253113\n",
      "loss:  0.252716\n",
      "loss:  0.252366\n",
      "loss:  0.252055\n",
      "loss:  0.251778\n",
      "loss:  0.251529\n",
      "loss:  0.251304\n",
      "loss:  0.251101\n",
      "loss:  0.250915\n",
      "loss:  0.250746\n",
      "loss:  0.250592\n",
      "loss:  0.250451\n",
      "loss:  0.250321\n",
      "loss:  0.250202\n",
      "loss:  0.250092\n",
      "loss:  0.249990\n",
      "loss:  0.249896\n",
      "loss:  0.249809\n",
      "loss:  0.249728\n",
      "loss:  0.249652\n",
      "loss:  0.249581\n",
      "loss:  0.249514\n",
      "loss:  0.249451\n",
      "loss:  0.249392\n",
      "loss:  0.249336\n",
      "loss:  0.249283\n",
      "loss:  0.249233\n",
      "loss:  0.249185\n",
      "loss:  0.249140\n",
      "loss:  0.249096\n",
      "loss:  0.249054\n",
      "loss:  0.249015\n",
      "loss:  0.248977\n",
      "loss:  0.248940\n",
      "loss:  0.248905\n",
      "loss:  0.248872\n",
      "\n",
      "************************************ training the 2st layer... ************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.422042\n",
      "loss:  0.384358\n",
      "loss:  0.372346\n",
      "loss:  0.365744\n",
      "loss:  0.361460\n",
      "loss:  0.357634\n",
      "loss:  0.351653\n",
      "loss:  0.348501\n",
      "loss:  0.346709\n",
      "loss:  0.345257\n",
      "loss:  0.344046\n",
      "loss:  0.343018\n",
      "loss:  0.342131\n",
      "loss:  0.341355\n",
      "loss:  0.340665\n",
      "loss:  0.340041\n",
      "loss:  0.339473\n",
      "loss:  0.338947\n",
      "loss:  0.338455\n",
      "loss:  0.337990\n",
      "loss:  0.337551\n",
      "loss:  0.337135\n",
      "loss:  0.336743\n",
      "loss:  0.336377\n",
      "loss:  0.336034\n",
      "loss:  0.335712\n",
      "loss:  0.335411\n",
      "loss:  0.335127\n",
      "loss:  0.334858\n",
      "loss:  0.334602\n",
      "loss:  0.334357\n",
      "loss:  0.334123\n",
      "loss:  0.333899\n",
      "loss:  0.333683\n",
      "loss:  0.333475\n",
      "loss:  0.333274\n",
      "loss:  0.333080\n",
      "loss:  0.332891\n",
      "loss:  0.332709\n",
      "loss:  0.332532\n",
      "loss:  0.332360\n",
      "loss:  0.332192\n",
      "loss:  0.332028\n",
      "loss:  0.331869\n",
      "loss:  0.331712\n",
      "loss:  0.331559\n",
      "loss:  0.331409\n",
      "loss:  0.331262\n",
      "loss:  0.331117\n",
      "loss:  0.330974\n",
      "\n",
      "************************************ training the dense layer... ************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14014682\n",
      "loss: 0.06379384\n",
      "loss: 0.052104417\n",
      "loss: 0.04184141\n",
      "loss: 0.03266477\n",
      "loss: 0.025707297\n",
      "loss: 0.02090253\n",
      "loss: 0.017246671\n",
      "loss: 0.015436205\n",
      "loss: 0.0143482005\n",
      "loss: 0.013593083\n",
      "loss: 0.0129238125\n",
      "loss: 0.012740462\n",
      "loss: 0.012600008\n",
      "loss: 0.012541166\n",
      "loss: 0.01236496\n",
      "loss: 0.012210192\n",
      "loss: 0.0120345075\n",
      "loss: 0.011945933\n",
      "loss: 0.011847492\n",
      "loss: 0.011786246\n",
      "loss: 0.011707326\n",
      "loss: 0.011586506\n",
      "loss: 0.0114905285\n",
      "loss: 0.011390453\n",
      "loss: 0.011327406\n",
      "loss: 0.011257047\n",
      "loss: 0.011197386\n",
      "loss: 0.011128413\n",
      "loss: 0.0111121535\n",
      "loss: 0.011090102\n",
      "loss: 0.011049976\n",
      "loss: 0.010996046\n",
      "loss: 0.010960062\n",
      "loss: 0.010929263\n",
      "loss: 0.010899158\n",
      "loss: 0.010889356\n",
      "loss: 0.010860745\n",
      "loss: 0.010825914\n",
      "loss: 0.010801095\n",
      "loss: 0.010755655\n",
      "loss: 0.010739739\n",
      "loss: 0.010721318\n",
      "loss: 0.010761224\n",
      "loss: 0.010784647\n",
      "loss: 0.010757613\n",
      "loss: 0.010717334\n",
      "loss: 0.010746299\n",
      "loss: 0.010641152\n",
      "\n",
      "************************************ fine turning... ************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012495918\n",
      "loss: 0.012144204\n",
      "loss: 0.010286344\n",
      "loss: 0.011173941\n",
      "loss: 0.010484284\n",
      "loss: 0.010048989\n",
      "loss: 0.0088680545\n",
      "loss: 0.009066064\n",
      "loss: 0.0088937245\n",
      "loss: 0.008761941\n",
      "loss: 0.008672219\n",
      "loss: 0.008352388\n",
      "loss: 0.008120507\n",
      "loss: 0.007892914\n",
      "loss: 0.007919688\n",
      "loss: 0.007723218\n",
      "loss: 0.0075300843\n",
      "loss: 0.007431918\n",
      "loss: 0.007350918\n",
      "loss: 0.0073308456\n",
      "loss: 0.0072574085\n",
      "loss: 0.007177391\n",
      "loss: 0.0071298755\n",
      "loss: 0.007234218\n",
      "loss: 0.0070494255\n",
      "loss: 0.0071013183\n",
      "loss: 0.0070583746\n",
      "loss: 0.0070140036\n",
      "loss: 0.006983112\n",
      "loss: 0.0077866875\n",
      "loss: 0.007741496\n",
      "loss: 0.0076924516\n",
      "loss: 0.006624778\n",
      "loss: 0.006553799\n",
      "loss: 0.006549256\n",
      "loss: 0.0064869067\n",
      "loss: 0.00654217\n",
      "loss: 0.007488854\n",
      "loss: 0.0074034687\n",
      "loss: 0.007342536\n",
      "loss: 0.0072991494\n",
      "loss: 0.0072677764\n",
      "loss: 0.007233916\n",
      "loss: 0.0071898624\n",
      "loss: 0.007158764\n",
      "loss: 0.0071231155\n",
      "loss: 0.0071135433\n",
      "loss: 0.007098741\n",
      "loss: 0.0070616044\n",
      "loss: 0.0070230663\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Save to path: ', '../models/SAE_all_beijing_1/save_net.ckpt')\n",
      "beijing predicting vs ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss: 0.0107466355\n",
      "\n",
      "275.5620343574105\n",
      "0.617198238994\n"
     ]
    }
   ],
   "source": [
    "city = 'beijing'\n",
    "learning_rate = 0.001\n",
    "car_id_batch_size = 5\n",
    "keep_probability = 1\n",
    "gap = 20\n",
    "predict_steps = 10\n",
    "n_hidden_1 = 56\n",
    "n_hidden_2 = 48\n",
    "n_dense = [128,128] # = []\n",
    "epochs = 500\n",
    "model_index = 1\n",
    "\n",
    "# encoder1st_model_index,\n",
    "# encoder2nd_model_index = 90\n",
    "load_model_index = None\n",
    "display_step = 10\n",
    "r_threshold = 200\n",
    "GPU_index = 0\n",
    "mu = 1\n",
    "\n",
    "n_input= 30*2\n",
    "car_id_batch = int(10000 / car_id_batch_size)\n",
    "model_name = 'SAE_all_'+ city + '_' +str(model_index)\n",
    "n_output= predict_steps*2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_index)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil,sqrt\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.allow_growth = True  # 设置tf模式为按需赠长模式\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "def distance(true_x,true_y,predict_x,predict_y):\n",
    "    return great_circle((true_x,true_y),(predict_x,predict_y)).m\n",
    "\n",
    "# 向量化\n",
    "vec_distance = np.vectorize(distance)\n",
    "\n",
    "print (city+'***********************************************************************************************')\n",
    "try:\n",
    "    cdf = pd.read_csv('/home/liheyuan/Jupyter/trajectory_prediction/trajectory_prediction_1/middata/'+city+'_tongyi_cdf1.csv',index_col=0)\n",
    "    ctdf = pd.read_csv('/home/liheyuan/Jupyter/trajectory_prediction/trajectory_prediction_1/middata/'+city+'_tongyi_ctdf1.csv',index_col=0)\n",
    "except:\n",
    "    cdf = pd.read_csv('/home/liheyuan/Jupyter/trajectory_prediction/trajectory_prediction_1/HMM/hmm4/middata/'\\\n",
    "                      +city+'_tongyi_cdf1.csv',index_col=0)\n",
    "    ctdf = pd.read_csv('/home/liheyuan/Jupyter/trajectory_prediction/trajectory_prediction_1/HMM/hmm4/middata/'\\\n",
    "                       +city+'_tongyi_ctdf1.csv',index_col=0)\n",
    "\n",
    "\n",
    "tmp_cdf = np.array([cdf.x,cdf.y]).T\n",
    "min_max_scaler = MinMaxScaler().fit(tmp_cdf)\n",
    "tmp_1 = min_max_scaler.transform(tmp_cdf)\n",
    "cdf['x'] = tmp_1[:,0]\n",
    "cdf['y'] = tmp_1[:,1]\n",
    "\n",
    "tmp_ctdf = np.array([ctdf.x,ctdf.y]).T\n",
    "tmp_2 = min_max_scaler.transform(tmp_ctdf)\n",
    "ctdf['x'] = tmp_2[:,0]\n",
    "ctdf['y'] = tmp_2[:,1]\n",
    "\n",
    "print ('data has been imported')\n",
    "\n",
    "weights = { \n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]),name='W_encoder_h1'),  \n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W_encoder_h2'),  \n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "}  \n",
    "biases = {  \n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1]),name='b_encoder_h1'),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2]),name='b_encoder_h2'),  \n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_input])),  \n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_hidden_1])),  \n",
    "} \n",
    "\n",
    "#************************* 1st hidden layer **************  \n",
    "\n",
    "X = tf.placeholder(\"float32\", [None, n_input])  \n",
    "y_ = tf.placeholder(\"float32\", shape = [None, n_output])  \n",
    "keep_prob = tf.placeholder(\"float32\")  \n",
    "\n",
    "# 编码器1\n",
    "h1_out =tf.nn.sigmoid(tf.add(tf.matmul(X, weights['encoder_h1']),biases['encoder_b1']))\n",
    "h1_out_drop = tf.nn.dropout(h1_out,keep_prob)\n",
    "\n",
    "# 解码器1  \n",
    "X_1 = tf.nn.sigmoid(tf.matmul(h1_out_drop,weights['decoder_h1'])+biases['decoder_b1'])  \n",
    "cross_entropy_1 = tf.reduce_mean(-tf.reduce_sum(X * tf.log(X_1),reduction_indices=[1]))\n",
    "original_loss_1 = tf.reduce_mean(tf.pow(X - X_1, 2))\n",
    "loss1 = original_loss_1 + mu * cross_entropy_1\n",
    "train_step_1 = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss1,var_list=[weights['encoder_h1'],biases['encoder_b1'],\n",
    "                                                                               weights['decoder_h1'],biases['decoder_b1']])  \n",
    "\n",
    "#************************** 2nd hidden layer *************  \n",
    "\n",
    "# 编码器2\n",
    "h2_out =tf.nn.sigmoid(tf.add(tf.matmul(h1_out_drop, weights['encoder_h2']),biases['encoder_b2']))\n",
    "h2_out_drop = tf.nn.dropout(h2_out,keep_prob)\n",
    "\n",
    "# 解码器2\n",
    "X_2 = tf.nn.sigmoid(tf.matmul(h2_out_drop,weights['decoder_h2'])+biases['decoder_b2'])  \n",
    "cross_entropy_2 = tf.reduce_mean(-tf.reduce_sum(h1_out_drop * tf.log(X_2),reduction_indices=[1]))\n",
    "original_loss_2 = tf.reduce_mean(tf.pow(h1_out_drop - X_2, 2))\n",
    "loss2 = original_loss_2 + mu * cross_entropy_2\n",
    "train_step_2 = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss2,var_list=[weights['encoder_h2'],biases['encoder_b2'],\n",
    "                                                                               weights['decoder_h2'],biases['decoder_b2']])\n",
    "\n",
    "#************************** dense layer *************  \n",
    "\n",
    "with tf.variable_scope('dense_layers'):\n",
    "    layer_3 = tf.layers.dense(inputs=h2_out_drop, units=128, activation=tf.nn.relu) \n",
    "    layer_4 = tf.layers.dense(inputs=layer_3, units=128, activation=tf.nn.relu) \n",
    "    logits = tf.layers.dense(inputs=layer_4, units=predict_steps*2)\n",
    "\n",
    "tmp = y_ - logits\n",
    "tmp_1 = tf.strided_slice(tmp,[0,0],[(799 - 30 - predict_steps)*car_id_batch_size,predict_steps*2],[1,2])\n",
    "tmp_2 = tf.strided_slice(tmp,[0,1],[(799 - 30 - predict_steps)*car_id_batch_size,predict_steps*2],[1,2])\n",
    "\n",
    "loss3 = tf.reduce_mean(tf.sqrt(tf.square(tmp_1)+tf.square(tmp_2)))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# *********************************** gradient check *********************************************\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# dense\n",
    "params_dense = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dense_layers')\n",
    "\n",
    "gradients_dense = tf.gradients(loss3, params_dense)\n",
    "\n",
    "checked_grad_dense = []\n",
    "for check in gradients_dense:\n",
    "    checked_grad_dense.append(tf.check_numerics(check,\"error occur\" ))\n",
    "\n",
    "with tf.control_dependencies(checked_grad_dense):\n",
    "    train_step_dense = opt.apply_gradients(zip(checked_grad_dense, params_dense))\n",
    "\n",
    "# fine\n",
    "params_ = params_dense[:]\n",
    "params_.extend([weights['encoder_h1'],biases['encoder_b1'],\n",
    "    weights['encoder_h2'],biases['encoder_b2']])\n",
    "\n",
    "gradients = tf.gradients(loss3, params_)\n",
    "\n",
    "checked_grad = []\n",
    "for check in gradients:\n",
    "    checked_grad.append(tf.check_numerics(check,\"error occur\" ))\n",
    "    \n",
    "with tf.control_dependencies(checked_grad):\n",
    "    train_step_3 = opt.apply_gradients(zip(checked_grad, params_))\n",
    "    \n",
    "    \n",
    "try:\n",
    "    sess = tf.Session(config=config)\n",
    "except:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1-GPU_index)\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "# #     SAE_encoder2nd\n",
    "# load_model = '../../SAE_2/models/SAE_encoder2nd_'+city+'_'+str(encoder2nd_model_index)+'/save_net.ckpt'\n",
    "# load_saver = tf.train.Saver({\"W_encoder_h1\":weights['encoder_h1'], \"b_encoder_h1\": biases['encoder_b1'],\n",
    "#                             \"W_encoder_h2\":weights['encoder_h2'], \"b_encoder_h2\": biases['encoder_b2']})\n",
    "# load_saver.restore(sess, load_model)\n",
    "# print 'SAE_encoder2nd load from '+ load_model\n",
    "\n",
    "\n",
    "# if load_model_index:\n",
    "#     load_model = '../models/SAE_encoder2nd_'+city+'_'+str(load_model_index)+'/save_net.ckpt'\n",
    "#     load_saver = tf.train.Saver()\n",
    "#     load_saver.restore(sess, load_model)\n",
    "#     print 'load from '+ load_model\n",
    "# else:\n",
    "#     print 'start a new model'\n",
    "\n",
    "print ('************************************ training the lst layer... ************************************')\n",
    "loss_list_1 = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    tmp_loss_list_1 = []\n",
    "    for car_id in range(car_id_batch):\n",
    "        fff = np.array([cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['x'],\n",
    "                        cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['y']]).T.reshape((car_id_batch_size,800,2))\n",
    "        batch_x = np.array([fff[:,start_point:start_point+30,:] for start_point in range(0,799 - 30 - predict_steps,predict_steps)])\\\n",
    "                  .reshape((int(ceil(((799 - 30 - predict_steps+1)*1.0/predict_steps)))*car_id_batch_size,30*2))\n",
    "        \n",
    "        _,c=sess.run([train_step_1,loss1],feed_dict={X:batch_x, keep_prob:keep_probability})\n",
    "        tmp_loss_list_1.append(c)\n",
    "    loss_list_1.append(np.mean(tmp_loss_list_1))\n",
    "    if (i+1) % display_step == 0:\n",
    "        print 'loss:%10f'%(loss_list_1[-1])\n",
    "        \n",
    "        \n",
    "print ('************************************ training the 2st layer... ************************************')\n",
    "loss_list_2 = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    tmp_loss_list_2 = []\n",
    "    for car_id in range(car_id_batch):\n",
    "        fff = np.array([cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['x'],\n",
    "                        cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['y']]).T.reshape((car_id_batch_size,800,2))\n",
    "        batch_x = np.array([fff[:,start_point:start_point+30,:] for start_point in range(0,799 - 30 - predict_steps,predict_steps)])\\\n",
    "                  .reshape((int(ceil(((799 - 30 - predict_steps+1)*1.0/predict_steps)))*car_id_batch_size,30*2))\n",
    "\n",
    "        _,c=sess.run([train_step_2,loss2],feed_dict={X:batch_x, keep_prob:keep_probability})\n",
    "        tmp_loss_list_2.append(c)\n",
    "    loss_list_2.append(np.mean(tmp_loss_list_2))\n",
    "    if (i+1) % display_step == 0:\n",
    "        print 'loss:%10f'%(loss_list_2[-1])\n",
    "\n",
    "\n",
    "print ('************************************ training the dense layer... ************************************')\n",
    "loss_list_3 = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    tmp_loss_list_3 = []\n",
    "    for car_id in range(car_id_batch):\n",
    "        fff = np.array([cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['x'],\n",
    "                        cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['y']]).T.reshape((car_id_batch_size,800,2))\n",
    "        batch_x = np.array([fff[:,start_point:start_point+30,:] for start_point in range(0,799 - 30 - gap,gap)][:-1])\\\n",
    "                  .reshape((int(ceil(((799 - 30 - gap+1)*1.0/gap))-1)*car_id_batch_size,30*2))\n",
    "        batch_y = np.array([fff[:,start_point:start_point+predict_steps,:] for start_point in range(29+1,799-gap,gap)][:-1])\\\n",
    "                  .reshape((int(ceil(((799 - 30 - gap+1)*1.0/gap))-1)*car_id_batch_size,predict_steps*2))\n",
    "        try: \n",
    "            _,loss = sess.run([train_step_dense,loss3],feed_dict={X:batch_x,y_:batch_y, keep_prob:keep_probability})\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            continue\n",
    "\n",
    "        tmp_loss_list_3.append(loss)\n",
    "    loss_list_3.append(np.mean(tmp_loss_list_3))\n",
    "    if (i+1) % display_step == 0:\n",
    "        print 'loss:',loss\n",
    "        \n",
    "        \n",
    "print ('************************************ fine turning... ************************************')\n",
    "loss_list_4 = []\n",
    "for i in tqdm(range(epochs)):\n",
    "    tmp_loss_list_4 = []\n",
    "    for car_id in range(car_id_batch):\n",
    "        fff = np.array([cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['x'],\n",
    "                        cdf.iloc[800*car_id:800*(car_id+car_id_batch_size)]['y']]).T.reshape((car_id_batch_size,800,2))\n",
    "        batch_x = np.array([fff[:,start_point:start_point+30,:] for start_point in range(0,799 - 30 - gap,gap)][:-1])\\\n",
    "                  .reshape((int(ceil(((799 - 30 - gap+1)*1.0/gap))-1)*car_id_batch_size,30*2))\n",
    "        batch_y = np.array([fff[:,start_point:start_point+predict_steps,:] for start_point in range(29+1,799-gap,gap)][:-1])\\\n",
    "                  .reshape((int(ceil(((799 - 30 - gap+1)*1.0/gap))-1)*car_id_batch_size,predict_steps*2))\n",
    "        try: \n",
    "            _,loss = sess.run([train_step_3,loss3],feed_dict={X:batch_x,y_:batch_y, keep_prob:keep_probability})\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            continue\n",
    "\n",
    "        tmp_loss_list_4.append(loss)\n",
    "    loss_list_4.append(np.mean(tmp_loss_list_4))\n",
    "    if (i+1) % display_step == 0:\n",
    "        print 'loss:',loss\n",
    "\n",
    "f = plt.figure(figsize=(16,5))\n",
    "plt.plot(loss_list_3)\n",
    "# plt.savefig('../images/loss_'+model_name+'.png')\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(16,5))\n",
    "plt.plot(loss_list_4)\n",
    "plt.show()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, '../models/'+model_name+'/save_net.ckpt')\n",
    "print(\"Save to path: \", save_path)\n",
    "\n",
    "print ('beijing predicting vs ...')\n",
    "\n",
    "predict_positions = []\n",
    "anwser_positions = []\n",
    "for car_id in tqdm(range(10000)):\n",
    "    ttt = np.array([ctdf.iloc[200*car_id:200*(car_id+1)]['x'],\n",
    "                    ctdf.iloc[200*car_id:200*(car_id+1)]['y']]).T.reshape((200,2))\n",
    "    batch_x = np.array([ttt[start_point:start_point+30,:] for start_point in range(199 - 30 - predict_steps)])\\\n",
    "              .reshape((199 - 30 - predict_steps,30*2))\n",
    "    batch_y = np.array([ttt[start_point:start_point+predict_steps,:] for start_point in range(29+1,199-predict_steps)])\\\n",
    "              .reshape((199 - 30 - predict_steps,predict_steps*2))\n",
    "    predict_positions.append(sess.run(logits,feed_dict={X:batch_x,keep_prob:1.0}))\n",
    "    anwser_positions.append(batch_y)\n",
    "\n",
    "predict_positions = np.array(predict_positions).reshape(10000*(199 - 30 - predict_steps)*predict_steps,2)\n",
    "anwser_positions = np.array(anwser_positions).reshape(10000*(199 - 30 - predict_steps)*predict_steps,2)\n",
    "\n",
    "# 还原经纬度，用于计算误差\n",
    "predict_positions = min_max_scaler.inverse_transform(predict_positions)\n",
    "anwser_positions = min_max_scaler.inverse_transform(anwser_positions)\n",
    "\n",
    "errors = vec_distance(predict_positions[:,0],predict_positions[:,1],anwser_positions[:,0],anwser_positions[:,1])\n",
    "mean_error = np.mean(errors)\n",
    "acc = len(errors[errors<=r_threshold]) * 1.0 / len(errors)\n",
    "\n",
    "print 'loss:',loss_list_3[-1]\n",
    "print ''\n",
    "print mean_error\n",
    "print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:46:28.423865Z",
     "start_time": "2018-04-03T14:45:27.743551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0032732289975660094\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def distance(true_x,true_y,predict_x,predict_y):\n",
    "    return sqrt((true_x-predict_x)**2+(true_y-predict_y)**2)\n",
    "\n",
    "# 向量化\n",
    "vec_distance_m = np.vectorize(distance)\n",
    "\n",
    "error_m = np.mean(vec_distance_m(predict_positions[:,0],predict_positions[:,1],anwser_positions[:,0],anwser_positions[:,1]))\n",
    "\n",
    "print error_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
